{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans 1:\n",
    "Overfitting is a concept in data science , which occurs when a statistical model fits exactly against its training data. When this happends, the algorithm unfortunately cannot perform against unseen data, defeating its purpose. When the model trains for too long on sample data or when the model is too complex, it can start to learn the \"noise\", or irrelevent information, within the dataset. When the model memorizes the noise and fits too closely to the training set, the model becomes \"overfitted\", and it is unable to generalize well to new data. Low error rates and high varience are good indicators of overfitting\n",
    "\n",
    "Consequences of overfitting: If a model cannot generalize to the new data, then it will not be able to perform the classification or prediction task that it was intended for.\n",
    "\n",
    "Following are the techniques to mitigate overfitting:\n",
    "1. Early stopping: Pause training before the model starts learning the noise within the model. This approach risks halting the training process too soon, leading to the opposite porblem of underfitting. Finding the \" sweet spot\" between overfitting and underfitting is the ultimate goal here.\n",
    "2. Data Augmentation: While it is better to inject clean, relevent data into your training, sometimes noisy data is added to make the model more stable. This should be done sparingly.\n",
    "3. Feature selection: While building a model, a number of features or parameters are used to predict a given outcome, but many times, these features can be redundant to others. Feature selection is the process of identifying the most important ones within the training data and then eleminating the irrelevant or redundant ones. \n",
    "\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate in both the training set and unseen data. It occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. High bias and low variance are good indicators of underfitting. Since this behaviour can be seen while using the training dataset, underfitting models are usually easier to identify then overfitted ones.\n",
    "\n",
    "Consequences of Underfitting: When a model is underfitted, it cannot stablish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize to the new data, then it cannot be leveraged for classification or prediction tasks.\n",
    "\n",
    "Following are the techniques to mitigate underfitiing:\n",
    "1. Decrease regularization: Regularizatin is typically used to reduce the varience with a model by applying a panelty to the inout parameters with the larger coefficients. there are a number of different methods, such ass L1 regularization, Lasso regularization, dropout, etc., which help to reduce the noise and outliers within a model. However, if the data feature becomes too unifrom, the model is unable to identify the dominant trend, leading to underfitting. By deccreasing the amount of  regularization, more complexity and variation is introduced into the model, allowing for successful training of the model.\n",
    "2. Increase the duration of training: Stopping the training of the model too soon can result in underfitting of the model , so it is important to train the model for optimum anount of time.\n",
    "3. Feature Selection: With any model, specific features are used to determine a given outcome. If there are not enough predictive features present, then more features or features with greater importance, should be introduced. For example, in a neural network, you might want to add more hidden neurons or in a random foresst, you may add more trees. This process will inject more complexity into the mmodel, Yielding better training results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans 2:\n",
    "Following are the techniques to reduce overfitting:\n",
    "1. Early stopping: Pause training before the model starts learning the noise within the model. This approach risks halting the training process too soon, leading to the opposite porblem of underfitting. Finding the \" sweet spot\" between overfitting and underfitting is the ultimate goal here.\n",
    "2. Data Augmentation: While it is better to inject clean, relevent data into your training, sometimes noisy data is added to make the model more stable. This should be done sparingly.\n",
    "3. Feature selection: While building a model, a number of features or parameters are used to predict a given outcome, but many times, these features can be redundant to others. Feature selection is the process of identifying the most important ones within the training data and then eleminating the irrelevant or redundant ones. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans3:\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate in both the training set and unseen data. It occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. High bias and low variance are good indicators of underfitting. Since this behaviour can be seen while using the training dataset, underfitting models are usually easier to identify then overfitted ones.\n",
    "\n",
    "Following can be reasons for underfitting:\n",
    "1. High bias an low variance\n",
    "2. The size of training dataset is not enough\n",
    "3. The model is too simple\n",
    "4. Training data is not clean\n",
    "5. Training time is not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
